{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSNfnawBYEie2CxHu9q/D6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lIJ5jE-2L0MG"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import random\n","\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","source":["VERBOSE = False"],"metadata":{"id":"PwALTODrCiKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initial value for experiment\n","omega_d = 50\n","omega = 15\n","C = 15\n","mu = 6\n","alpha = 0.7\n","d_0 = 5\n","R_0 = 1"],"metadata":{"id":"xn-ls0NUL7XQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# steady state of mouse experiment\n","R = R_0\n","d_st = d_0\n","\n","# calculate g steady state\n","g_st = (C - d_0 + mu * np.log(R))/alpha\n","\n","g_st"],"metadata":{"id":"Q9FlePzVMQUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ode_system(R, d_val, g_val, omega_d, C, mu, alpha, omega, d0):\n","    d_dot = omega_d * (C + mu * np.log(R) - alpha * g_val - d_val)\n","    g_dot = omega * (d_val / d0 - 1)\n","    return [d_dot, g_dot]"],"metadata":{"id":"TSdb6lLGB6sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# using Euler method\n","def explicit_euler(old_d_val, old_g_val, R_input, h=0.01):\n","\n","    new_d_val = old_d_val + h * ode_system(R_input, old_d_val, old_g_val, omega_d, C, mu, alpha, omega, d_0)[0]\n","    new_g_val = old_g_val + h * ode_system(R_input, old_d_val, old_g_val, omega_d, C, mu, alpha, omega, d_0)[1]\n","\n","    return new_d_val, new_g_val"],"metadata":{"id":"BmjhEzb2O0OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Updated parameters for increased sensitivity\n","m0 = 1\n","alpha = 0.1\n","n = 1\n","h = 1\n","a0 = 1/3\n","tau = 3"],"metadata":{"id":"iSDO6t_9g85T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_a_l_m(l, m):\n","    K_m = np.exp(alpha * (m - m0))\n","    a = 1 / (1 + (l / K_m)**n)\n","    return a\n","\n","def calculate_phi(l,m):\n","    l+=1\n","    a_lm = calculate_a_l_m(l,m)\n","    value = 1/tau * (a_lm/a0)**h\n","    return  1-max(0, min(0.9, value))"],"metadata":{"id":"5W_eMEhyg-M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GridEnvironment:\n","    def __init__(self, width, height, reward_pos_size_prob):\n","        self.width = width\n","        self.height = height\n","        self.reward_pos, self.reward_size, self.reward_prob = reward_pos_size_prob\n","        self.visits = np.zeros((width, height))\n","\n","    def step(self, action, state):\n","        # Update state based on action\n","        #action0 = right 1=up 2=left 3=down\n","        state_m, state_n = state\n","\n","        if action == 0:\n","            state_n += 1\n","        elif action == 1:\n","            state_m -= 1\n","        elif action == 2:\n","            state_n -= 1\n","        elif action == 3:\n","            state_m += 1\n","        else:\n","            raise NotImplementedError('Wrong action')\n","\n","        state_m = max(0, min(state_m, self.height - 1)) #make sure the agent stays in the grid\n","        state_n = max(0, min(state_n, self.width - 1)) #make sure the agent stays in the grid\n","\n","        self.visits[state_m][state_n] += 1\n","\n","        reward = 0\n","        for i in range(len(self.reward_pos)):\n","            # only at the reward position and with certain probability,the reward appeared\n","            if self.reward_pos[i][0] == state_m and self.reward_pos[i][1] == state_n and np.random.random() <= self.reward_prob[i]:\n","                    reward = self.reward_size[i]\n","                    break\n","\n","        state = [state_m, state_n]\n","\n","        return state, reward\n","\n","def update_dopamine(d_list, g_list, reward_input):\n","    dop, gaba = explicit_euler(d_list[-1], g_list[-1], reward_input+1)\n","    return dop, gaba\n"],"metadata":{"id":"GX2nF9p06jFH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TDLearningAgent:\n","    def __init__(self, num_states_m, num_states_n, num_actions, learning_rate, gamma):\n","        self.q_table = np.zeros((num_states_m, num_states_n, num_actions)) # agent expect no reward\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","\n","    def choose_action(self, prev_action, state, epsilon, last_epi=False):\n","        if last_epi:\n","            return 1\n","\n","        if np.random.rand() < epsilon:\n","\n","            elements = [0, 1, 2, 3]\n","\n","            # Filter the list to exclude the prev_direction\n","            filtered_elements = [elem for elem in elements if elem != prev_action]\n","\n","            # Randomly choose an element from the filtered list\n","            new_direction = random.choice(filtered_elements)\n","\n","            return new_direction  # change direction with prob epsilon\n","\n","        else:\n","            return prev_action # else keep direction\n","\n","    def learn(self, state, action, reward):\n","\n","        state_m, state_n = state\n","\n","        q_action = action\n","\n","        if action == 0:\n","            target = reward + self.gamma * np.max(self.q_table[state_m, state_n+1])\n","        elif action == 1:\n","            target = reward + self.gamma * np.max(self.q_table[state_m-1, state_n])\n","        elif action == 2:\n","            target = reward + self.gamma * np.max(self.q_table[state_m, state_n-1])\n","        elif action == 3:\n","            target = reward + self.gamma * np.max(self.q_table[state_m+1, state_n])\n","        else:\n","            raise NotImplementedError('Wrong action')\n","\n","        predict = self.q_table[state_m, state_n, q_action]\n","\n","        updated_value = self.q_table[state_m, state_n, q_action] + self.learning_rate * (target - predict)\n","        self.q_table[state_m, state_n, q_action] = max(0, updated_value)\n"],"metadata":{"id":"alxmSTaJAVro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def flatten_index(m, n, width):\n","    \"\"\" Convert 2D grid coordinates to a flattened index. \"\"\"\n","    return m * width + n\n","\n","def flatten_visits(visit_matrix):\n","    \"\"\" Flatten a 2D matrix of visits into a 1D list. \"\"\"\n","    return list(np.array(visit_matrix).flatten())\n","\n","def plot_visits(visit_matrix, reward_pos, width, height):\n","    \"\"\"\n","    Plot the cumulative frequency of an agent's positions on a grid, emphasizing reward positions.\n","\n","    Parameters:\n","    - visit_matrix: 2D list or numpy array, where element [m][n] contains the number of visits to position (m, n).\n","    - reward_pos: List of tuples, where each tuple is (m, n) coordinates of a reward position.\n","    - width, height: Dimensions of the grid.\n","    \"\"\"\n","    # Flatten the visit matrix to a 1D list of visits\n","    visits = flatten_visits(visit_matrix)\n","    positions = list(range(width * height))  # List of all position indices\n","\n","    # Generate labels for the x-axis using grid coordinates\n","    labels = [f\"({i // width},{i % width})\" for i in positions]\n","\n","    plt.figure(figsize=(15, 6))\n","    plt.bar(positions, visits, color='gray', label='Visits')\n","\n","    # Highlight reward positions by plotting them in a different color\n","    reward_indices = [flatten_index(m, n, width) for m, n in reward_pos]\n","    reward_visits = [visits[idx] for idx in reward_indices]\n","    plt.bar(reward_indices, reward_visits, color='red', label='Reward Positions')\n","\n","    plt.xlabel('Position (m, n)', fontsize=14)\n","    plt.ylabel('Number of Visits', fontsize=14)\n","    #plt.title('Cumulative Frequency of Agent\\'s Positions with Reward Emphasis')\n","    plt.xticks(positions, labels, rotation=45, ha='right')  # Rotate labels for better readability\n","\n","    plt.legend()\n","    plt.grid(True)  # Adds a grid for better visibility\n","\n","    if VERBOSE:\n","        directory = f'drive/MyDrive/Dopamine/graphs/'\n","        name = f'grid_cumu_freq_sto_rew_dop_reori_td'\n","        plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","    plt.show()"],"metadata":{"id":"b2_kGDh6Kvz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_visits_heatmap(visit_matrix, reward_pos, width, height):\n","    \"\"\"\n","    Plot a heatmap of the cumulative frequency of an agent's positions on a grid,\n","    emphasizing reward positions with a specific marker.\n","\n","    Parameters:\n","    - visit_matrix: 2D list or numpy array, where element [m][n] contains the number of visits to position (m, n).\n","    - reward_pos: List of tuples, where each tuple is (m, n) coordinates of a reward position.\n","    - width, height: Dimensions of the grid.\n","    \"\"\"\n","    visits = np.array(visit_matrix)  # Convert visit matrix to numpy array for better handling in plotting\n","\n","    plt.figure(figsize=(8, 8))\n","    ax = plt.gca()  # Get the current axis\n","\n","    # Create a heatmap\n","    cax = ax.imshow(visits, cmap='hot', interpolation='nearest', aspect='equal')\n","\n","    # Adding color bar\n","    cbar = plt.colorbar(cax, orientation='vertical')\n","    cbar.set_label('Number of Visits', fontsize=14)\n","\n","    # Mark reward positions\n","    for m, n in reward_pos:\n","        plt.scatter(n, m, color='cyan', s=200, edgecolors='black', marker='o')  # Circle marker for reward positions\n","\n","    # Set axis properties\n","    ax.set_xticks(np.arange(width))\n","    ax.set_yticks(np.arange(height))\n","    ax.set_xticklabels(np.arange(width))\n","    ax.set_yticklabels(np.arange(height))\n","    ax.set_xlabel('State Dimension n', fontsize=14)\n","    ax.set_ylabel('State Dimension m', fontsize=14)\n","    #ax.set_title('Heatmap of Agent\\'s Visit Frequencies')\n","\n","    # Minor ticks to show grid lines\n","    ax.set_xticks(np.arange(-.5, width, 1), minor=True)\n","    ax.set_yticks(np.arange(-.5, height, 1), minor=True)\n","    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n","\n","    if VERBOSE:\n","        directory = f'drive/MyDrive/Dopamine/graphs/'\n","        name = f'grid_heat_sto_rew_dop_reori_td'\n","        plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","    plt.show()\n"],"metadata":{"id":"u14SSX-rD8AZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialization\n","rew_list = [[[2,3], [3,2], [4,4]], [1,2,4], [1,0.5, 0.25]]\n","env = GridEnvironment(width=7, height=7, reward_pos_size_prob=rew_list)\n","agent = TDLearningAgent(num_states_m=7, num_states_n=7, num_actions=4, learning_rate=0.1, gamma=0.2)\n","\n","d_list_whole = []\n","g_list_whole = []\n","q_values_over_time = []\n","\n","# Training Loop\n","for episode in tqdm(range(20000)):\n","    state = [0,0]\n","    done = False\n","    total_reward = 0\n","    d_list = [d_0]\n","    g_list = [g_st]\n","    pred_error_list=[]\n","    action = 0 #init action\n","\n","    reward = 0\n","    dop_spike=0\n","\n","    while not done:\n","\n","        agent.learn(state, action, reward)\n","\n","        state_m, state_n = state\n","\n","        epsilon = calculate_phi(dop_spike, d_0)\n","\n","        if state_m == env.height - 2 and state_n == env.width - 2:\n","            done = True\n","        elif state_m == 0 and state_n == 0:\n","            action = random.choice([0,3])\n","        else:\n","            if state_m == env.height - 2:\n","                action = random.choice([0,1,2])\n","            elif state_n == env.width - 2:\n","                action = random.choice([1,2,3])\n","            elif state_n == 0:\n","                action = random.choice([0,1,3])\n","            elif state_m == 0:\n","                action = random.choice([0,2,3])\n","            else:\n","                action = agent.choose_action(action, state, epsilon)\n","\n","        next_state, reward = env.step(action, state)\n","\n","        q_action = action\n","\n","        # Update Dopamine based on prediction error\n","        prediction_error = abs(reward + agent.gamma * np.max(agent.q_table[next_state[0],next_state[1]]) - agent.q_table[next_state[0],next_state[1], q_action])\n","\n","        dop, gaba = update_dopamine(d_list, g_list, prediction_error)\n","        dop_spike = dop - d_list[0]\n","        d_list.append(dop)\n","        g_list.append(gaba)\n","\n","        state = next_state\n","\n","        if next_state[0] == env.height - 1 and next_state[1] == env.width - 1:\n","            done = True\n","\n","    if episode % 10 == 0:\n","        d_list_whole.append(d_list)\n","        g_list_whole.append(g_list)\n","        q_values_over_time.append(agent.q_table[:].copy())"],"metadata":{"id":"0A2LzIyCAVro"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialization\n","env_simu = GridEnvironment(width=7, height=7, reward_pos_size_prob=rew_list)\n","\n","d_list_simu = []\n","g_list_simu = []\n","q_values_over_time_simu = []\n","\n","finish = False\n","\n","state = [0,0]\n","action = 0 #init action\n","reward = 0\n","dop_spike=0\n","d_list = [d_0]\n","g_list = [g_st]\n","\n","hit_time = 0\n","\n","# Simulation Loop\n","while not finish:\n","\n","    state_m, state_n = state\n","\n","    epsilon = calculate_phi(dop_spike, d_0)\n","\n","    if state_m == env_simu.height - 1 and state_n == env_simu.width - 1:\n","        action = random.choice([1,2])\n","        hit_time += 1\n","    elif state_m == 0 and state_n == 0:\n","        hit_time += 1\n","        action = random.choice([0,3])\n","    else:\n","        if state_m == env_simu.height - 1:\n","            action = random.choice([0,1,2])\n","        elif state_n == env_simu.width - 1:\n","            action = random.choice([1,2,3])\n","        elif state_n == 0:\n","            action = random.choice([0,1,3])\n","        elif state_m == 0:\n","            action = random.choice([0,2,3])\n","        else:\n","            action = agent.choose_action(action, state, epsilon)\n","\n","    next_state, reward = env_simu.step(action, state)\n","\n","    agent.learn(state, action, reward)\n","\n","    q_action = action\n","\n","    # Update Dopamine based on prediction error\n","    prediction_error = abs(reward + agent.gamma * np.max(agent.q_table[next_state[0],next_state[1]]) - agent.q_table[next_state[0],next_state[1], q_action])\n","\n","    dop, gaba = update_dopamine(d_list, g_list, prediction_error)\n","    dop_spike = dop - d_list[0]\n","    d_list.append(dop)\n","    g_list.append(gaba)\n","\n","    state = next_state\n","\n","    if hit_time >= 10000:\n","        finish = True\n"],"metadata":{"id":"qY_-e-GJLevR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_visits(env_simu.visits, env_simu.reward_pos, 7,7)"],"metadata":{"id":"dZsyjiCILd87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_visits_heatmap(env_simu.visits, env_simu.reward_pos, 7,7)"],"metadata":{"id":"ciLodVYdD_vA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q_values_over_time = np.array(q_values_over_time)\n","actions = range(4)\n","\n","fig, axs = plt.subplots(2, 2, figsize=(20, 12), subplot_kw={'projection': '3d'})\n","axs = axs.ravel()\n","\n","for i, ax in enumerate(axs):\n","    if i < len(actions):\n","        action_index = actions[i]\n","        Q_values = q_values_over_time[-1, :, :, action_index]\n","\n","        x = np.arange(Q_values.shape[1])\n","        y = np.arange(Q_values.shape[0])\n","        X, Y = np.meshgrid(x, y)\n","\n","        # Plot the surface\n","        surface = ax.plot_surface(X, Y, Q_values, cmap='viridis', alpha=0.7)\n","\n","        # Customize each subplot\n","        for j in range(len(rew_list[0])):\n","            m, n = rew_list[0][j]\n","            reward_expected_value = rew_list[1][j] * rew_list[2][j]\n","            ax.scatter(n, m, reward_expected_value, s=10 * rew_list[1][j], depthshade=True)\n","            ax.plot([n, n], [m, m], [0, 2], color='red', linestyle='dotted', linewidth=2)\n","\n","        # Axis labels and title\n","        ax.set_xlabel('State Dimension n', labelpad=10)\n","        ax.set_ylabel('State Dimension m', labelpad=10)\n","        ax.set_zlabel('Q-values', labelpad=10)\n","        ax.set_title(f'Action {action_index}')\n","\n","        # View angle for better visibility\n","        ax.view_init(elev=20, azim=-30)\n","\n","plt.subplots_adjust(left=0.05, right=0.85, top=0.95, bottom=0.05, wspace=0.2, hspace=0.1)\n","\n","# Add a color bar\n","cbar_ax = fig.add_axes([0.88, 0.15, 0.03, 0.7])\n","fig.colorbar(surface, cax=cbar_ax, label='Q-value')\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'grid_last_q_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"2zkdcCvxLywx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["width, height = 7, 7\n","\n","fig, axes = plt.subplots(1, len(rew_list[0]), figsize=(18, 6), sharey=True)\n","\n","colors = ['red', 'blue', 'green', 'purple']\n","labels = ['Action 0', 'Action 1', 'Action 2', 'Action 3']\n","\n","all_q_values = []\n","\n","for i in range(3):\n","    m = rew_list[0][i][0]\n","    n = rew_list[0][i][1]\n","    all_q_values.extend(q_values_over_time[:, m, n, :].flatten())\n","\n","min_q_value = min(all_q_values)\n","max_q_value = max(all_q_values)\n","\n","bin_width = 0.05\n","bins = np.arange(min_q_value, max_q_value + bin_width, bin_width)\n","\n","for index, (m, n) in enumerate(rew_list[0]):\n","    data = [q_values_over_time[:, m, n, action] for action in range(4)]\n","    axes[index].hist(data, bins=bins, color=colors, label=labels, stacked=True, alpha=0.75)\n","\n","    axes[index].set_xlabel('Q-value intervals', fontsize=14)\n","    axes[index].set_xlim(min_q_value, max_q_value)\n","    axes[index].set_title(f'Reward at Position ({m},{n}) with Size {rew_list[1][index]} and Probability {rew_list[2][index]}')\n","\n","    expected_value = rew_list[1][index] * rew_list[2][index]\n","    axes[index].axvline(x=expected_value, color='black', linestyle='--', linewidth=2, label=f'Expected Value = {expected_value}')\n","    axes[index].legend(loc='upper right')\n","\n","fig.text(0.02, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=12)\n","\n","plt.tight_layout(rect=[0.03, 0, 1, 0.95])\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'grid_q_freq_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"zSHITPEAuxts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yqbRBknKzabm"},"execution_count":null,"outputs":[]}]}