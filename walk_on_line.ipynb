{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM7/qCH8LlFx+xEuv7MblWy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lIJ5jE-2L0MG"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import random\n","\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","SEED = 42\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","source":["VERBOSE = False"],"metadata":{"id":"PwALTODrCiKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# initial value for experiment\n","omega_d = 50\n","omega = 15\n","C = 15\n","mu = 6\n","alpha = 0.7\n","d_0 = 5\n","R_0 = 1"],"metadata":{"id":"xn-ls0NUL7XQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# steady state of mouse experiment\n","R = R_0\n","d_st = d_0\n","\n","# calculate g steady state\n","g_st = (C - d_0 + mu * np.log(R))/alpha\n","\n","g_st"],"metadata":{"id":"Q9FlePzVMQUn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ode_system(R, d_val, g_val, omega_d, C, mu, alpha, omega, d0):\n","    d_dot = omega_d * (C + mu * np.log(R) - alpha * g_val - d_val)\n","    g_dot = omega * (d_val / d0 - 1)\n","    return [d_dot, g_dot]"],"metadata":{"id":"TSdb6lLGB6sn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# using Euler method\n","def explicit_euler(old_d_val, old_g_val, R_input, h=0.01):\n","\n","    new_d_val = old_d_val + h * ode_system(R_input, old_d_val, old_g_val, omega_d, C, mu, alpha, omega, d_0)[0]\n","    new_g_val = old_g_val + h * ode_system(R_input, old_d_val, old_g_val, omega_d, C, mu, alpha, omega, d_0)[1]\n","\n","    return new_d_val, new_g_val\n"],"metadata":{"id":"BmjhEzb2O0OS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Updated parameters for increased sensitivity\n","m0 = 1\n","alpha = 0.1\n","n = 1\n","h = 1\n","a0 = 1/3\n","tau = 3"],"metadata":{"id":"iSDO6t_9g85T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_a_l_m(l, m):\n","    K_m = np.exp(alpha * (m - m0))\n","    a = 1 / (1 + (l / K_m)**n)\n","    return a\n","\n","def calculate_phi(l,m):\n","    l+=1\n","    a_lm = calculate_a_l_m(l,m)\n","    value = 1/tau * (a_lm/a0)**h\n","    return  1-max(0, min(0.9, value))"],"metadata":{"id":"5W_eMEhyg-M2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LineEnvironment:\n","    def __init__(self, length, reward_pos_size_prob):\n","        self.length = length\n","        self.reward_pos, self.reward_size, self.reward_prob = reward_pos_size_prob\n","        self.visits = [0] * length # Initialize a list to record visits to each position\n","\n","    def step(self, action, state):\n","        # Update state based on action\n","        state = max(0, min(state + action, self.length - 1))\n","        self.visits[state] += 1  # Record the visit to the current state\n","\n","        reward = 0\n","        for i in range(len(self.reward_pos)):\n","            if self.reward_pos[i] == state and np.random.random() <= self.reward_prob[i]:\n","                reward = self.reward_size[i]\n","                break\n","\n","        return state, reward\n","\n","def update_dopamine(d_list, g_list, reward_input):\n","    dop, gaba = explicit_euler(d_list[-1], g_list[-1], reward_input+1)\n","    return dop, gaba"],"metadata":{"id":"x3EUMRecoe7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TDLearningAgent:\n","    def __init__(self, num_states, num_actions, learning_rate, gamma):\n","        self.q_table = np.zeros((num_states, num_actions)) # agent expect no reward\n","        self.learning_rate = learning_rate\n","        self.gamma = gamma\n","\n","    def choose_action(self, prev_action, state, epsilon, last_epi=False):\n","        if last_epi:\n","            return 1\n","\n","        if np.random.rand() < epsilon:\n","            return -1*prev_action  # change direction with prob epsilon\n","        else:\n","            return prev_action # else keep direction\n","\n","    def learn(self, state, action, reward):\n","\n","        if action == -1:\n","            q_action = 0\n","        else:\n","            q_action = 1\n","\n","        predict = self.q_table[state, q_action]\n","        next_state = state + action\n","        target = reward + self.gamma * np.max(self.q_table[next_state])\n","        updated_value = self.q_table[state, q_action] + self.learning_rate * (target - predict)\n","        self.q_table[state, q_action] = max(0, updated_value)"],"metadata":{"id":"WEG8IsD9oe7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_visits(visits, reward_pos):\n","    visits[0] = 0\n","    positions = list(range(len(visits)))\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(positions, visits, color='gray', label='Visits')\n","\n","    reward_visits = [visits[pos] for pos in reward_pos]\n","    plt.bar(reward_pos, reward_visits, color='red', label='Reward Positions')\n","\n","    plt.xlabel('Position', fontsize=14)\n","    plt.ylabel('Number of Visits', fontsize=14)\n","\n","    plt.xticks(positions, [str(pos) for pos in positions])\n","\n","    plt.legend()\n","    plt.grid(True)\n","\n","    if VERBOSE:\n","        directory = f'drive/MyDrive/Dopamine/graphs/'\n","        name = f'cumu_freq_sto_rew_dop_reori_td'\n","        plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","    plt.show()"],"metadata":{"id":"UIlaInf6qeUg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialization\n","rew_list = [[5, 9, 13], [1,2,4], [1, 0.5, 0.25]]\n","env = LineEnvironment(length=20, reward_pos_size_prob=rew_list)\n","agent = TDLearningAgent(num_states=20, num_actions=2, learning_rate=0.1, gamma=0.3)\n","epsilon = 0\n","\n","d_list_whole = []\n","g_list_whole = []\n","q_values_over_time = []\n","\n","# Training Loop\n","for episode in tqdm(range(10000)):\n","    state = 0\n","    done = False\n","    total_reward = 0\n","    d_list = [d_0]\n","    g_list = [g_st]\n","    pred_error_list=[]\n","    action = 1 #init action\n","\n","    reward = 0\n","    dop_spike=0\n","\n","    while not done:\n","        agent.learn(state, action, reward)\n","\n","        epsilon = calculate_phi(dop_spike, d_0)\n","\n","        action = agent.choose_action(action, state, epsilon)\n","        next_state, reward = env.step(action, state)\n","\n","        if action == -1:\n","                q_action = 0\n","        else:\n","                q_action = 1\n","\n","        # Update Dopamine based on prediction error\n","        prediction_error = abs(reward + agent.gamma * np.max(agent.q_table[next_state + action]) - agent.q_table[next_state, q_action])\n","        dop, gaba = update_dopamine(d_list, g_list, prediction_error)\n","        dop_spike = dop - d_list[0]\n","        d_list.append(dop)\n","        g_list.append(gaba)\n","\n","        state = next_state\n","\n","        if state == env.length - 2:  # Agent reaches the end of the line\n","            done = True\n","        if state == 0: # Agent go back to the start of the line, go right again\n","            action = 1\n","\n","    if episode % 5 == 0:\n","        d_list_whole.append(d_list)\n","        g_list_whole.append(g_list)\n","        q_values_over_time.append(agent.q_table[:].copy())\n"],"metadata":{"id":"BlR5xIzioe7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Run the simulation\n","# Initialization\n","env_simu = LineEnvironment(length=20, reward_pos_size_prob=rew_list)\n","\n","d_list_simu = []\n","g_list_simu = []\n","q_values_over_simu = []\n","\n","hit_time = 0\n","state = 0\n","finish = False\n","action = 1 #init action\n","reward = 0\n","dop_spike=0\n","\n","# Simulation Loop\n","while not finish:\n","\n","    agent.learn(state, action, reward)\n","\n","    epsilon = calculate_phi(dop_spike, d_0)\n","\n","    action = agent.choose_action(action, state, epsilon)\n","\n","    if state == env_simu.length - 2:\n","        action = -1  # Agent reaches the end of the line\n","        hit_time += 1\n","\n","    if state == 0: # Agent go back to the start of the line, go right again\n","        action = 1\n","        hit_time += 1\n","\n","    next_state, reward = env_simu.step(action, state)\n","\n","    if action == -1:\n","            q_action = 0\n","    else:\n","            q_action = 1\n","\n","    # Update Dopamine based on prediction error\n","    prediction_error = abs(reward + agent.gamma * np.max(agent.q_table[next_state + action]) - agent.q_table[next_state, q_action])\n","    dop, gaba = update_dopamine(d_list, g_list, prediction_error)\n","    dop_spike = dop - d_list[0]\n","    d_list.append(dop)\n","    g_list.append(gaba)\n","\n","    state = next_state\n","\n","    if hit_time > 10000:\n","        finish = True\n","\n","plot_visits(env_simu.visits, env_simu.reward_pos)\n"],"metadata":{"id":"PvVOgm_BsTnD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# last episode, at the end, run a deterministic simulation where the agent simply goes in one direction\n","state = 0\n","done = False\n","d_list = [d_0]\n","g_list = [g_st]\n","action = 1 #init action\n","reward=0\n","\n","state_list = [0]\n","\n","rew_list_final = [[5, 9, 13], [1,0,0], [1, 0, 0]]\n","env_final = LineEnvironment(length=20, reward_pos_size_prob=rew_list_final)\n","\n","while not done:\n","    agent.learn(state, action, reward)\n","\n","    action = agent.choose_action(action, state, epsilon=0, last_epi=True)\n","    next_state, reward = env_final.step(action, state)\n","\n","    if action == -1:\n","            q_action = 0\n","    else:\n","            q_action = 1\n","\n","    # Update Dopamine based on prediction error\n","    prediction_error = max(0, reward + agent.gamma * np.max(agent.q_table[next_state + action]) - agent.q_table[next_state, q_action])\n","    dop, gaba = update_dopamine(d_list, g_list, prediction_error)\n","    dop_spike = dop - d_list[0]\n","    d_list.append(dop)\n","    g_list.append(gaba)\n","\n","    state = next_state\n","\n","    if state == env_final.length - 2:  # Agent reaches the end of the line\n","            done = True\n","    if state == 0: # Agent go back to the start of the line, go right again\n","            action = 1\n","\n","d_list_whole.append(d_list)\n","g_list_whole.append(g_list)\n","q_values_over_time.append(agent.q_table[:].copy())"],"metadata":{"id":"XkpU6MaTsQHW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["q_values_over_time = np.array(q_values_over_time)"],"metadata":{"id":"BdUU7DrGoe7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","\n","for i in range(0, len(q_values_over_time), 400):\n","    plt.plot(q_values_over_time[i, :, 1], label=f'Q table at epoch {i*5}')\n","\n","# Draw vertical lines at reward positions\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.axvline(x=rew_pos, color='red', linestyle='--', linewidth=0.8, label='Reward Position' if rew_pos == rew_list[0][0] else None)\n","\n","# Draw horizontal lines from x=0 to each reward position with the corresponding reward size\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.hlines(y=rew_size*rew_prob, xmin=0, xmax=rew_pos, color='blue', linestyle='--', linewidth=0.8, label=f'Expectation of Reward' if rew_pos == rew_list[0][0] else None)\n","\n","plt.xlabel('Position', fontsize=14)\n","plt.ylabel('Q-values', fontsize=14)\n","\n","plt.legend()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'Q_Evolution1_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"Lw_KZ0_Uoe7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","\n","for i in range(0, len(q_values_over_time), 400):\n","    plt.plot(q_values_over_time[i, :, 0], label=f'Q table at epoch {i*5}')\n","\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.axvline(x=rew_pos, color='red', linestyle='--', linewidth=0.8, label='Reward Position' if rew_pos == rew_list[0][0] else None)\n","\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.hlines(y=rew_size*rew_prob, xmin=0, xmax=rew_pos, color='blue', linestyle='--', linewidth=0.8, label=f'Expectation of Reward' if rew_pos == rew_list[0][0] else None)\n","\n","plt.xlabel('Position', fontsize=14)\n","plt.ylabel('Q-values', fontsize=14)\n","\n","plt.legend(loc='upper left')\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'Q_Evolution-1_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"3gOmY1RRoe7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 6))\n","\n","plt.plot(q_values_over_time[-2, :, 1], label=f'Q table for action 1')\n","plt.plot(q_values_over_time[-2, :, 0], label=f'Q table for action -1')\n","\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.axvline(x=rew_pos, color='red', linestyle='--', linewidth=0.8, label='Reward Position' if rew_pos == rew_list[0][0] else None)\n","\n","for rew_pos, rew_size, rew_prob in zip(rew_list[0], rew_list[1], rew_list[2]):\n","    plt.hlines(y=rew_size*rew_prob, xmin=0, xmax=rew_pos, color='blue', linestyle='--', linewidth=0.8, label=f'Expectation of Reward' if rew_pos == rew_list[0][0] else None)\n","\n","plt.xlabel('Position', fontsize=14)\n","plt.ylabel('Q-values', fontsize=14)\n","\n","plt.legend()\n","\n","plt.tight_layout(rect=[0, 0, 0.75, 1])\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'last_Q_1-1_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"4HQC00uroe7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True, sharex=True)\n","\n","all_q_values = []\n","for rew_pos in rew_list[0]:\n","    all_q_values.extend(q_values_over_time[:, rew_pos, 1])\n","\n","min_q_value = min(all_q_values)\n","max_q_value = max(all_q_values)\n","\n","bin_width = 0.05\n","\n","bins = np.arange(min_q_value, max_q_value + bin_width, bin_width)\n","\n","for index, rew_pos in enumerate(rew_list[0][:3]):\n","    rew_size = rew_list[1][index]\n","    rew_prob = rew_list[2][index]\n","    q_values_at_pos1 = q_values_over_time[:, rew_pos, 1]\n","    q_values_at_pos0 = q_values_over_time[:, rew_pos, 0]\n","\n","    axes[index].hist(q_values_at_pos0, bins=bins, color='red', alpha=0.7, label='Action -1')\n","    axes[index].hist(q_values_at_pos1, bins=bins, color='blue', alpha=0.7, label='Action 1')\n","    axes[index].set_xlabel('Q-value intervals', fontsize=14)\n","    axes[index].set_xlim(min_q_value, max_q_value)\n","    axes[index].set_title(f'Reward at position {rew_pos} with size {rew_list[1][index]} and probability {rew_list[2][index]}')\n","    axes[index].axvline(x=rew_size*rew_prob, color='blue', linestyle='--', linewidth=1, label=f'Expectation = {rew_size*rew_prob}')\n","    axes[index].legend()\n","\n","fig.text(0.02, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=12)\n","\n","\n","plt.tight_layout(rect=[0.03, 0, 1, 0.95])\n","\n","if VERBOSE:\n","    directory = 'drive/MyDrive/Dopamine/graphs/'\n","    plt.savefig(directory + 'q_freq_sto_rew_dop_reori_td.png', format='png', bbox_inches='tight', dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"tZ9W5rk5oe7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["last_episode_dopamine = d_list_whole[-1]\n","last_episode_gaba = g_list_whole[-1]\n","\n","fig, axes = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n","\n","axes[0].plot(last_episode_dopamine, color='blue', label='Dopamine Level at last epoch')\n","axes[0].set_ylabel('Dopamine Level', fontsize=14)\n","axes[0].set_ylim([4.95, 5.15])\n","for rew_pos in rew_list[0]:\n","    axes[0].axvline(x=rew_pos, color='red', linestyle='--', linewidth=0.8, label='Reward Position' if rew_pos == rew_list[0][0] else None)\n","axes[0].axhline(y=d_st, color='blue', linestyle='--', linewidth=0.8, label='Dopamine Steady state')\n","\n","formatter = ScalarFormatter(useOffset=False)\n","formatter.set_scientific(False)\n","axes[0].yaxis.set_major_formatter(formatter)\n","\n","axes[0].legend()\n","\n","axes[1].plot(last_episode_gaba, color='green', label='GABA Level at last epoch')\n","axes[1].set_ylabel('GABA Level', fontsize=14)\n","axes[1].set_ylim([14.28, 14.30])\n","axes[1].set_xlabel('Position', fontsize=14)\n","for rew_pos in rew_list[0]:\n","    axes[1].axvline(x=rew_pos, color='red', linestyle='--', linewidth=0.8, label='Reward Position' if rew_pos == rew_list[0][0] else None)\n","axes[1].axhline(y=g_st, color='blue', linestyle='--', linewidth=0.8, label='GABA Steady state')\n","\n","axes[1].legend()\n","\n","axes[1].yaxis.set_major_formatter(formatter)\n","\n","plt.tight_layout(pad=1.0)\n","fig.subplots_adjust(right=0.75)\n","\n","if VERBOSE:\n","    directory = f'drive/MyDrive/Dopamine/graphs/'\n","    name = f'last_dop_gaba_sto_rew_dop_reori_td'\n","    plt.savefig(directory + name +'.png', format='png',bbox_inches='tight',dpi=500)\n","\n","plt.show()\n"],"metadata":{"id":"IlyoG90ioe7B"},"execution_count":null,"outputs":[]}]}